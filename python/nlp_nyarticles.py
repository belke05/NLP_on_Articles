# -*- coding: utf-8 -*-
"""nlp_NYarticles.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bgLkewdRHT-9bnmLSS2-3XOCRMl5vFbb
"""

import pandas as pd, matplotlib.pyplot as plt, seaborn as sns, numpy as np
import re 
from nltk.tokenize import regexp_tokenize, word_tokenize
from nltk.corpus import stopwords
from collections import Counter
from nltk.stem import WordNetLemmatizer
from gensim.corpora.dictionary import Dictionary
import nltk 
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

from google.colab import drive
drive.mount('/content/drive')

republican_articles = pd.read_csv('/content/republican.csv')
democrat_articles = pd.read_csv('/content/democrat.csv')
wordnet_lemmatizer = WordNetLemmatizer()

# Lemmatize all tokens into a new list: lemmatized

republican_articles['mentionsTrump'] = republican_articles.abstract.apply(lambda x: 1 if 'trump' in x.lower() else 0)
democrat_articles['mentionsTrump'] = democrat_articles.abstract.apply(lambda x: 1 if 'trump' in x.lower() else 0)

republican_articles.mentionsTrump.value_counts()

democrat_articles.mentionsTrump.value_counts()

# emoji = "['\U0001F300-\U0001F5FF'|'\U0001F600-\U0001F64F'|'\U0001F680-\U0001F6FF'|'\u2600-\u26FF\u2700-\u27BF']"
# print(regexp_tokenize(german_text, emoji))

def wordReturned(abstract):
  wordsonly = [ word for word in word_tokenize(abstract.lower()) if word.isalpha()]
  return wordsonly

def noStopWordsReturned(wordsonly):
  nostops = [word for word in wordsonly if word not in stopwords.words('english')]
  return nostops

def lemmatizing(nostops):
  lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in nostops]
  return lemmatized

def cleanup(abstract):
  wordsonly = wordReturned(abstract)
  nostop =  noStopWordsReturned(wordsonly)
  return lemmatizing(nostop)

republican_articles['abstract_tokens'] = republican_articles.abstract.apply(lambda abstract: cleanup(abstract))
democrat_articles['abstract_tokens'] = democrat_articles.abstract.apply(lambda abstract: cleanup(abstract))
republican_articles['token_count'] = republican_articles.abstract_tokens.apply(lambda x: Counter(x))
democrat_articles['token_count'] = democrat_articles.abstract_tokens.apply(lambda x: Counter(x))

republican_articles.head()

allwords_republicans = []
republican_articles.abstract_tokens.apply(lambda x: allwords_republicans.extend(x))
print(allwords_republicans)
print(Counter(allwords_republicans))
print(Counter(allwords_republicans).most_common(20))

allwords_democrats = []
democrat_articles.abstract_tokens.apply(lambda x: allwords_democrats.extend(x))
print(allwords_democrats)
print(Counter(allwords_democrats))
print(Counter(allwords_democrats).most_common(20))

def change(doc):
  lowdoc = doc.lower()
  tokenized_docs = [word_tokenize(lowdoc)]
  dict_tokens = Dictionary(tokenized_docs)
  return dict_tokens.token2id
def corpus(doc):
  lowdoc = doc.lower()
  tokenized_docs = [word_tokenize(lowdoc)]
  dict_tokens = Dictionary(tokenized_docs)
  corpus = [dict_tokens.doc2bow(doc) for doc in tokenized_docs]
  return corpus

republican_articles['token_dictionary'] = republican_articles.abstract.apply(lambda doc: change(doc)) 
republican_articles['corpus'] = republican_articles.abstract.apply(lambda doc: corpus(doc))

republican_articles.head()

